{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=r'..\\data\\processed\\new_data.pickle'\n",
    "MLFLOW_TRACKING_URI=\"..\\models\\mlruns\"\n",
    "MLFLOW_EXPERIMENT_NAME = \"Amazon_products_recommendation_system\"\n",
    "LOG_PATH = \"../models/temp/\"\n",
    "LOG_DATA_PKL    =  \"data.pkl\"\n",
    "LOG_MODEL_PKL   =  \"model.pkl\"\n",
    "LOG_METRICS_PKL =  \"metrics.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import mlflow\n",
    "# from mlflow import MlflowClient\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Product_ID</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>A3LDPF5FMB782Z</td>\n",
       "      <td>1400501466</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321</th>\n",
       "      <td>A1A5KUIIIHFF4U</td>\n",
       "      <td>1400501466</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>A2XIOXRRYX0KZY</td>\n",
       "      <td>1400501466</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>AW3LX47IHPFRL</td>\n",
       "      <td>1400501466</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>A1E3OB6QMBKRYZ</td>\n",
       "      <td>1400501466</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7824422</th>\n",
       "      <td>A34BZM6S9L7QI4</td>\n",
       "      <td>B00LGQ6HL8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7824423</th>\n",
       "      <td>A1G650TTTHEAL5</td>\n",
       "      <td>B00LGQ6HL8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7824424</th>\n",
       "      <td>A25C2M3QF9G7OQ</td>\n",
       "      <td>B00LGQ6HL8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7824425</th>\n",
       "      <td>A1E1LEVQ9VQNK</td>\n",
       "      <td>B00LGQ6HL8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7824426</th>\n",
       "      <td>A2NYK9KWFMJV4Y</td>\n",
       "      <td>B00LGQ6HL8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62893 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                User_ID  Product_ID  Ratings  year  month\n",
       "1309     A3LDPF5FMB782Z  1400501466      5.0  2012      5\n",
       "1321     A1A5KUIIIHFF4U  1400501466      1.0  2012      3\n",
       "1334     A2XIOXRRYX0KZY  1400501466      3.0  2013      6\n",
       "1450      AW3LX47IHPFRL  1400501466      5.0  2012      6\n",
       "1455     A1E3OB6QMBKRYZ  1400501466      1.0  2012     10\n",
       "...                 ...         ...      ...   ...    ...\n",
       "7824422  A34BZM6S9L7QI4  B00LGQ6HL8      5.0  2014      7\n",
       "7824423  A1G650TTTHEAL5  B00LGQ6HL8      5.0  2014      7\n",
       "7824424  A25C2M3QF9G7OQ  B00LGQ6HL8      5.0  2014      7\n",
       "7824425   A1E1LEVQ9VQNK  B00LGQ6HL8      5.0  2014      7\n",
       "7824426  A2NYK9KWFMJV4Y  B00LGQ6HL8      5.0  2014      7\n",
       "\n",
       "[62893 rows x 5 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_pickle(DATA_PATH)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Product_ID</th>\n",
       "      <th>1400501466</th>\n",
       "      <th>1400532655</th>\n",
       "      <th>9983891212</th>\n",
       "      <th>B00000DM9W</th>\n",
       "      <th>B00000J1V5</th>\n",
       "      <th>B00000JDF5</th>\n",
       "      <th>B00000JDF6</th>\n",
       "      <th>B00000K135</th>\n",
       "      <th>B00000K4KH</th>\n",
       "      <th>B00001P4XA</th>\n",
       "      <th>...</th>\n",
       "      <th>B00KNM763E</th>\n",
       "      <th>B00KONCDVM</th>\n",
       "      <th>B00KVNY2KA</th>\n",
       "      <th>B00KXAFYZS</th>\n",
       "      <th>B00KYMCJF8</th>\n",
       "      <th>B00L21HC7A</th>\n",
       "      <th>B00L2442H0</th>\n",
       "      <th>B00L26YDA4</th>\n",
       "      <th>B00L3YHF6O</th>\n",
       "      <th>B00LGQ6HL8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A100UD67AHFODS</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A100WO06OQR8BQ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A105S56ODHGJEK</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A105TOJ6LTVMBG</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A10AFVU66A79Y1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZBXKUH4AIW3X</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZCE11PSTCH1L</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZMY6E8B52L2T</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZNUHQSHZHSUE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZOK5STV85FBJ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1466 rows × 5523 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Product_ID      1400501466  1400532655  9983891212  B00000DM9W  B00000J1V5  \\\n",
       "User_ID                                                                      \n",
       "A100UD67AHFODS         0.0         0.0         0.0         0.0         0.0   \n",
       "A100WO06OQR8BQ         0.0         0.0         0.0         0.0         0.0   \n",
       "A105S56ODHGJEK         0.0         0.0         0.0         0.0         0.0   \n",
       "A105TOJ6LTVMBG         0.0         0.0         0.0         0.0         4.0   \n",
       "A10AFVU66A79Y1         0.0         0.0         0.0         0.0         0.0   \n",
       "...                    ...         ...         ...         ...         ...   \n",
       "AZBXKUH4AIW3X          0.0         0.0         0.0         0.0         0.0   \n",
       "AZCE11PSTCH1L          0.0         0.0         0.0         0.0         0.0   \n",
       "AZMY6E8B52L2T          0.0         0.0         0.0         0.0         0.0   \n",
       "AZNUHQSHZHSUE          0.0         0.0         0.0         0.0         0.0   \n",
       "AZOK5STV85FBJ          0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "Product_ID      B00000JDF5  B00000JDF6  B00000K135  B00000K4KH  B00001P4XA  \\\n",
       "User_ID                                                                      \n",
       "A100UD67AHFODS         0.0         0.0         0.0         0.0         0.0   \n",
       "A100WO06OQR8BQ         0.0         0.0         0.0         0.0         0.0   \n",
       "A105S56ODHGJEK         0.0         0.0         0.0         0.0         0.0   \n",
       "A105TOJ6LTVMBG         0.0         0.0         0.0         0.0         0.0   \n",
       "A10AFVU66A79Y1         0.0         0.0         0.0         0.0         0.0   \n",
       "...                    ...         ...         ...         ...         ...   \n",
       "AZBXKUH4AIW3X          0.0         0.0         0.0         0.0         0.0   \n",
       "AZCE11PSTCH1L          0.0         0.0         0.0         0.0         0.0   \n",
       "AZMY6E8B52L2T          0.0         0.0         0.0         0.0         0.0   \n",
       "AZNUHQSHZHSUE          0.0         0.0         0.0         0.0         0.0   \n",
       "AZOK5STV85FBJ          0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "Product_ID      ...  B00KNM763E  B00KONCDVM  B00KVNY2KA  B00KXAFYZS  \\\n",
       "User_ID         ...                                                   \n",
       "A100UD67AHFODS  ...         0.0         0.0         0.0         0.0   \n",
       "A100WO06OQR8BQ  ...         0.0         0.0         0.0         0.0   \n",
       "A105S56ODHGJEK  ...         0.0         0.0         0.0         0.0   \n",
       "A105TOJ6LTVMBG  ...         0.0         0.0         0.0         0.0   \n",
       "A10AFVU66A79Y1  ...         0.0         0.0         0.0         0.0   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "AZBXKUH4AIW3X   ...         0.0         0.0         0.0         0.0   \n",
       "AZCE11PSTCH1L   ...         0.0         0.0         0.0         0.0   \n",
       "AZMY6E8B52L2T   ...         0.0         0.0         5.0         5.0   \n",
       "AZNUHQSHZHSUE   ...         0.0         0.0         0.0         0.0   \n",
       "AZOK5STV85FBJ   ...         0.0         0.0         0.0         0.0   \n",
       "\n",
       "Product_ID      B00KYMCJF8  B00L21HC7A  B00L2442H0  B00L26YDA4  B00L3YHF6O  \\\n",
       "User_ID                                                                      \n",
       "A100UD67AHFODS         0.0         0.0         0.0         0.0         0.0   \n",
       "A100WO06OQR8BQ         0.0         0.0         0.0         0.0         0.0   \n",
       "A105S56ODHGJEK         0.0         0.0         0.0         0.0         0.0   \n",
       "A105TOJ6LTVMBG         0.0         0.0         0.0         0.0         0.0   \n",
       "A10AFVU66A79Y1         0.0         0.0         0.0         0.0         0.0   \n",
       "...                    ...         ...         ...         ...         ...   \n",
       "AZBXKUH4AIW3X          0.0         0.0         0.0         0.0         0.0   \n",
       "AZCE11PSTCH1L          0.0         0.0         0.0         0.0         0.0   \n",
       "AZMY6E8B52L2T          0.0         5.0         5.0         5.0         0.0   \n",
       "AZNUHQSHZHSUE          0.0         0.0         0.0         0.0         0.0   \n",
       "AZOK5STV85FBJ          0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "Product_ID      B00LGQ6HL8  \n",
       "User_ID                     \n",
       "A100UD67AHFODS         0.0  \n",
       "A100WO06OQR8BQ         0.0  \n",
       "A105S56ODHGJEK         0.0  \n",
       "A105TOJ6LTVMBG         0.0  \n",
       "A10AFVU66A79Y1         0.0  \n",
       "...                    ...  \n",
       "AZBXKUH4AIW3X          0.0  \n",
       "AZCE11PSTCH1L          0.0  \n",
       "AZMY6E8B52L2T          0.0  \n",
       "AZNUHQSHZHSUE          0.0  \n",
       "AZOK5STV85FBJ          0.0  \n",
       "\n",
       "[1466 rows x 5523 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item=df.pivot(index='User_ID',columns='Product_ID',values='Ratings').fillna(0)\n",
    "user_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1466, 5523)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Reducing to 100 latent dimensions\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "user_embeddings = svd.fit_transform(user_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.53330541,  4.40360228, -1.53052736, ...,  1.18090294,\n",
       "         0.03301559,  0.05778452],\n",
       "       [ 6.94130631, -1.92579534, -1.67357295, ..., -1.54670264,\n",
       "        -1.3552569 , -0.44802601],\n",
       "       [ 6.34439439, -4.10044608,  2.6961316 , ...,  1.15551302,\n",
       "        -0.48327295,  1.51746706],\n",
       "       ...,\n",
       "       [ 9.6277111 , 17.81652055, -0.36660201, ..., -0.20895098,\n",
       "        -1.74434536,  4.81743923],\n",
       "       [ 2.47535058, -0.32131661, -0.82123224, ...,  1.77949437,\n",
       "         0.3649639 , -1.45024398],\n",
       "       [ 0.93023304,  0.43188796, -1.2040544 , ...,  0.48162532,\n",
       "        -0.50897649,  0.55024604]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00096706,  0.00092971,  0.00316971, ...,  0.0194871 ,\n",
       "         0.03023553,  0.00997189],\n",
       "       [-0.00052815,  0.00111599,  0.0012699 , ...,  0.05146367,\n",
       "         0.04333964,  0.03368475],\n",
       "       [-0.00060992, -0.00214826, -0.00825974, ...,  0.02118831,\n",
       "         0.01503552,  0.01637248],\n",
       "       ...,\n",
       "       [-0.00169206,  0.0020079 , -0.00643174, ..., -0.01054749,\n",
       "        -0.02524288, -0.00094783],\n",
       "       [ 0.00042355, -0.0005518 , -0.00919848, ..., -0.00032323,\n",
       "         0.00805532, -0.00407286],\n",
       "       [-0.00315683, -0.00498021, -0.00096713, ..., -0.0078656 ,\n",
       "        -0.01891403,  0.01231645]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(user_embeddings@svd.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5523)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_embeddings\n",
    "svd.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,latent_dim,profile_dim):\n",
    "        super(Generator,self).__init__()\n",
    "        self.model=nn.Sequential(\n",
    "            nn.Linear(latent_dim,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,profile_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward (self,z) :\n",
    "        return self.model(z)   \n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,profile_dim):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.model=nn.Sequential(\n",
    "\n",
    "            nn.Linear(profile_dim,256),\n",
    "            nn.LeakyReLU(.2),\n",
    "            nn.Linear(256,128),\n",
    "            nn.LeakyReLU(.2),\n",
    "            nn.Linear(128,1),\n",
    "            nn.Sigmoid()\n",
    "        )    \n",
    "\n",
    "    def forward(self,profile):\n",
    "        return self.model(profile)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100  # Dimension of random noise\n",
    "profile_dim = 5523  # Profile feature dimension\n",
    "batch_size = 64\n",
    "learning_rate = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator=Generator(latent_dim,profile_dim)\n",
    "discrimnator=Discriminator(profile_dim)\n",
    "\n",
    "criterion=nn.BCELoss()\n",
    "optimizer_G=optim.Adam(generator.parameters(),lr=learning_rate)\n",
    "optimizer_D=optim.Adam(discrimnator.parameters(),lr=learning_rate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_profiles=torch.tensor(user_embeddings)\n",
    "# real_profiles.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohamed Mosaad\\AppData\\Local\\Temp\\ipykernel_17560\\3866072594.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  real_profiles=torch.tensor(real_profiles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1000], d_loss: 0.014634043909609318, g_loss: 8.627960205078125\n",
      "Epoch [100/1000], d_loss: 0.004645331297069788, g_loss: 8.456727027893066\n",
      "Epoch [200/1000], d_loss: 0.00338601297698915, g_loss: 8.714179992675781\n",
      "Epoch [300/1000], d_loss: 0.03172688186168671, g_loss: 13.886958122253418\n",
      "Epoch [400/1000], d_loss: 0.0019262657733634114, g_loss: 11.01513385772705\n",
      "Epoch [500/1000], d_loss: 0.006052048411220312, g_loss: 9.920281410217285\n",
      "Epoch [600/1000], d_loss: 0.0032842871733009815, g_loss: 7.094478130340576\n",
      "Epoch [700/1000], d_loss: 0.0014289512764662504, g_loss: 8.171209335327148\n",
      "Epoch [800/1000], d_loss: 0.0009938082657754421, g_loss: 9.358089447021484\n",
      "Epoch [900/1000], d_loss: 0.000467195495730266, g_loss: 11.117900848388672\n"
     ]
    }
   ],
   "source": [
    "num_epochs=1000\n",
    "for epoch in range (num_epochs):\n",
    "    optimizer_D.zero_grad()\n",
    "    real_profiles = torch.tensor(user_item.values, dtype=torch.float32)  # Shape: (num_users, profile_dim)\n",
    "\n",
    "    real_profiles=torch.tensor(real_profiles)\n",
    "    real_lapels=torch.ones(real_profiles.size(0),1)\n",
    "    real_outputs=discrimnator(real_profiles)\n",
    "    d_loss_real=criterion(real_outputs,real_lapels)\n",
    "\n",
    "    z=torch.randn(real_profiles.size(0),latent_dim)\n",
    "    fake_profiles=generator(z)\n",
    "    fake_labels=torch.zeros(real_profiles.size(0),1)\n",
    "    fake_outputs=discrimnator(fake_profiles)\n",
    "    d_loss_fake=criterion(fake_outputs,fake_labels)\n",
    "\n",
    "    d_loss=d_loss_fake+d_loss_real\n",
    "    d_loss.backward(retain_graph=True)\n",
    "    optimizer_D.step()\n",
    "\n",
    "\n",
    "    optimizer_G.zero_grad()\n",
    "    fake_outputs=discrimnator( fake_profiles)\n",
    "    # print(fake_outputs)\n",
    "    g_loss=criterion(fake_outputs,real_lapels)\n",
    "    g_loss.backward()\n",
    "    optimizer_G.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], d_loss: {d_loss.item()}, g_loss: {g_loss.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5513</th>\n",
       "      <th>5514</th>\n",
       "      <th>5515</th>\n",
       "      <th>5516</th>\n",
       "      <th>5517</th>\n",
       "      <th>5518</th>\n",
       "      <th>5519</th>\n",
       "      <th>5520</th>\n",
       "      <th>5521</th>\n",
       "      <th>5522</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 5523 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2     3     4     5     6     7     8     9     ...  5513  \\\n",
       "0    0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "1    0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "2    0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "3    0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "4    0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "5    0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "6    0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "7    0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "8    0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "9    0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "10   0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "11   0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "12   0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "13   0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "14   0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "15   0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "16   0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "17   0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "18   0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "19   0.0   5.0   5.0   5.0   0.0   0.0   0.0   3.0   0.0   2.0  ...   0.0   \n",
       "\n",
       "    5514  5515  5516  5517  5518  5519  5520  5521  5522  \n",
       "0    0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1    0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2    0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3    0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4    0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "5    0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "6    0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "7    0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "8    0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "9    0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "10   0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "11   0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "12   0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "13   0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "14   0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "15   0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "16   0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "17   0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "18   0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "19   0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[20 rows x 5523 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=torch.randn(20,100)\n",
    "synthetic_profiles=generator(z)\n",
    "scaled_profiles = synthetic_profiles * 5  # Map [0, 1] to [0, 5]\n",
    "discrete_profiles = torch.round(scaled_profiles)\n",
    "\n",
    "# Threshold low values to 0 if needed\n",
    "threshold = 1\n",
    "discrete_profiles[discrete_profiles < threshold] = 0\n",
    "pd.DataFrame(torch.detach(discrete_profiles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1582, 1.0201, 1.0195,  ..., 0.9646, 1.0073, 1.0522],\n",
       "        [1.1478, 1.0322, 1.0232,  ..., 0.9743, 1.0116, 1.0559],\n",
       "        [1.1606, 1.0421, 1.0299,  ..., 0.9753, 1.0112, 1.0558],\n",
       "        ...,\n",
       "        [1.1548, 1.0279, 1.0248,  ..., 0.9739, 1.0047, 1.0492],\n",
       "        [1.1370, 1.0273, 1.0242,  ..., 0.9839, 1.0071, 1.0480],\n",
       "        [1.0858, 1.0215, 1.0081,  ..., 1.0058, 1.0080, 1.0354]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_profiles = (synthetic_profiles + 1) \n",
    "\n",
    "discrete_profiles = torch.round(scaled_profiles)\n",
    "threshold = 1\n",
    "discrete_profiles[discrete_profiles < threshold] = 0\n",
    "scaled_profiles\n",
    "# # Convert to a DataFrame for easier handling, with product IDs as columns\n",
    "# synthetic_profiles_df = pd.DataFrame(\n",
    "#     discrete_profiles.detach().numpy(), \n",
    "#     columns=[f\"Product_{i}\" for i in range(profile_dim)]\n",
    "# )\n",
    "# synthetic_profiles_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_0</th>\n",
       "      <th>Product_1</th>\n",
       "      <th>Product_2</th>\n",
       "      <th>Product_3</th>\n",
       "      <th>Product_4</th>\n",
       "      <th>Product_5</th>\n",
       "      <th>Product_6</th>\n",
       "      <th>Product_7</th>\n",
       "      <th>Product_8</th>\n",
       "      <th>Product_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Product_5513</th>\n",
       "      <th>Product_5514</th>\n",
       "      <th>Product_5515</th>\n",
       "      <th>Product_5516</th>\n",
       "      <th>Product_5517</th>\n",
       "      <th>Product_5518</th>\n",
       "      <th>Product_5519</th>\n",
       "      <th>Product_5520</th>\n",
       "      <th>Product_5521</th>\n",
       "      <th>Product_5522</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.500146</td>\n",
       "      <td>2.483446</td>\n",
       "      <td>2.545823</td>\n",
       "      <td>2.430334</td>\n",
       "      <td>2.418919</td>\n",
       "      <td>2.560656</td>\n",
       "      <td>2.573986</td>\n",
       "      <td>2.335510</td>\n",
       "      <td>2.598732</td>\n",
       "      <td>2.702518</td>\n",
       "      <td>...</td>\n",
       "      <td>2.423273</td>\n",
       "      <td>2.607817</td>\n",
       "      <td>2.725234</td>\n",
       "      <td>2.447518</td>\n",
       "      <td>2.378659</td>\n",
       "      <td>2.194475</td>\n",
       "      <td>2.133805</td>\n",
       "      <td>2.163052</td>\n",
       "      <td>2.176616</td>\n",
       "      <td>2.261167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.499887</td>\n",
       "      <td>2.491398</td>\n",
       "      <td>2.550073</td>\n",
       "      <td>2.457689</td>\n",
       "      <td>2.431810</td>\n",
       "      <td>2.535469</td>\n",
       "      <td>2.550894</td>\n",
       "      <td>2.375421</td>\n",
       "      <td>2.583809</td>\n",
       "      <td>2.640193</td>\n",
       "      <td>...</td>\n",
       "      <td>2.455241</td>\n",
       "      <td>2.574931</td>\n",
       "      <td>2.679621</td>\n",
       "      <td>2.489161</td>\n",
       "      <td>2.409468</td>\n",
       "      <td>2.319759</td>\n",
       "      <td>2.261487</td>\n",
       "      <td>2.276363</td>\n",
       "      <td>2.296059</td>\n",
       "      <td>2.308084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.501173</td>\n",
       "      <td>2.499612</td>\n",
       "      <td>2.542211</td>\n",
       "      <td>2.478822</td>\n",
       "      <td>2.458534</td>\n",
       "      <td>2.520449</td>\n",
       "      <td>2.532574</td>\n",
       "      <td>2.424906</td>\n",
       "      <td>2.574032</td>\n",
       "      <td>2.589723</td>\n",
       "      <td>...</td>\n",
       "      <td>2.480690</td>\n",
       "      <td>2.550498</td>\n",
       "      <td>2.616248</td>\n",
       "      <td>2.502605</td>\n",
       "      <td>2.441787</td>\n",
       "      <td>2.390606</td>\n",
       "      <td>2.357120</td>\n",
       "      <td>2.358881</td>\n",
       "      <td>2.381980</td>\n",
       "      <td>2.369528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.498349</td>\n",
       "      <td>2.495599</td>\n",
       "      <td>2.544780</td>\n",
       "      <td>2.485741</td>\n",
       "      <td>2.461424</td>\n",
       "      <td>2.529714</td>\n",
       "      <td>2.545595</td>\n",
       "      <td>2.413802</td>\n",
       "      <td>2.555633</td>\n",
       "      <td>2.597207</td>\n",
       "      <td>...</td>\n",
       "      <td>2.473763</td>\n",
       "      <td>2.547965</td>\n",
       "      <td>2.631993</td>\n",
       "      <td>2.505312</td>\n",
       "      <td>2.443451</td>\n",
       "      <td>2.412105</td>\n",
       "      <td>2.353026</td>\n",
       "      <td>2.371088</td>\n",
       "      <td>2.388363</td>\n",
       "      <td>2.356236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.500267</td>\n",
       "      <td>2.501994</td>\n",
       "      <td>2.526376</td>\n",
       "      <td>2.500614</td>\n",
       "      <td>2.485562</td>\n",
       "      <td>2.520482</td>\n",
       "      <td>2.522143</td>\n",
       "      <td>2.472629</td>\n",
       "      <td>2.543218</td>\n",
       "      <td>2.556836</td>\n",
       "      <td>...</td>\n",
       "      <td>2.498905</td>\n",
       "      <td>2.526371</td>\n",
       "      <td>2.564127</td>\n",
       "      <td>2.508245</td>\n",
       "      <td>2.482816</td>\n",
       "      <td>2.474928</td>\n",
       "      <td>2.453501</td>\n",
       "      <td>2.452916</td>\n",
       "      <td>2.480318</td>\n",
       "      <td>2.416341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.499891</td>\n",
       "      <td>2.501807</td>\n",
       "      <td>2.535126</td>\n",
       "      <td>2.494161</td>\n",
       "      <td>2.487670</td>\n",
       "      <td>2.521840</td>\n",
       "      <td>2.527334</td>\n",
       "      <td>2.471783</td>\n",
       "      <td>2.548614</td>\n",
       "      <td>2.571597</td>\n",
       "      <td>...</td>\n",
       "      <td>2.493630</td>\n",
       "      <td>2.527498</td>\n",
       "      <td>2.582798</td>\n",
       "      <td>2.511891</td>\n",
       "      <td>2.470459</td>\n",
       "      <td>2.461424</td>\n",
       "      <td>2.435823</td>\n",
       "      <td>2.438302</td>\n",
       "      <td>2.471032</td>\n",
       "      <td>2.404418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.499861</td>\n",
       "      <td>2.500766</td>\n",
       "      <td>2.527890</td>\n",
       "      <td>2.504906</td>\n",
       "      <td>2.499042</td>\n",
       "      <td>2.527144</td>\n",
       "      <td>2.529876</td>\n",
       "      <td>2.480635</td>\n",
       "      <td>2.533073</td>\n",
       "      <td>2.563903</td>\n",
       "      <td>...</td>\n",
       "      <td>2.500032</td>\n",
       "      <td>2.526649</td>\n",
       "      <td>2.567344</td>\n",
       "      <td>2.513847</td>\n",
       "      <td>2.485996</td>\n",
       "      <td>2.490911</td>\n",
       "      <td>2.462953</td>\n",
       "      <td>2.467316</td>\n",
       "      <td>2.493640</td>\n",
       "      <td>2.420988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.500702</td>\n",
       "      <td>2.502642</td>\n",
       "      <td>2.527015</td>\n",
       "      <td>2.501036</td>\n",
       "      <td>2.490412</td>\n",
       "      <td>2.520514</td>\n",
       "      <td>2.523102</td>\n",
       "      <td>2.477982</td>\n",
       "      <td>2.544531</td>\n",
       "      <td>2.554900</td>\n",
       "      <td>...</td>\n",
       "      <td>2.500430</td>\n",
       "      <td>2.525951</td>\n",
       "      <td>2.560450</td>\n",
       "      <td>2.510499</td>\n",
       "      <td>2.479122</td>\n",
       "      <td>2.477944</td>\n",
       "      <td>2.457366</td>\n",
       "      <td>2.457349</td>\n",
       "      <td>2.483157</td>\n",
       "      <td>2.417714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.497859</td>\n",
       "      <td>2.490908</td>\n",
       "      <td>2.552183</td>\n",
       "      <td>2.464734</td>\n",
       "      <td>2.440772</td>\n",
       "      <td>2.537108</td>\n",
       "      <td>2.552458</td>\n",
       "      <td>2.385446</td>\n",
       "      <td>2.570580</td>\n",
       "      <td>2.637084</td>\n",
       "      <td>...</td>\n",
       "      <td>2.450376</td>\n",
       "      <td>2.561986</td>\n",
       "      <td>2.678980</td>\n",
       "      <td>2.493556</td>\n",
       "      <td>2.414977</td>\n",
       "      <td>2.342915</td>\n",
       "      <td>2.284178</td>\n",
       "      <td>2.301177</td>\n",
       "      <td>2.324464</td>\n",
       "      <td>2.306963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.501272</td>\n",
       "      <td>2.502552</td>\n",
       "      <td>2.527914</td>\n",
       "      <td>2.497245</td>\n",
       "      <td>2.485324</td>\n",
       "      <td>2.520929</td>\n",
       "      <td>2.523123</td>\n",
       "      <td>2.473194</td>\n",
       "      <td>2.547755</td>\n",
       "      <td>2.557995</td>\n",
       "      <td>...</td>\n",
       "      <td>2.499270</td>\n",
       "      <td>2.529712</td>\n",
       "      <td>2.568072</td>\n",
       "      <td>2.510451</td>\n",
       "      <td>2.474978</td>\n",
       "      <td>2.469494</td>\n",
       "      <td>2.449777</td>\n",
       "      <td>2.447256</td>\n",
       "      <td>2.476366</td>\n",
       "      <td>2.412649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 5523 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Product_0  Product_1  Product_2  Product_3  Product_4  Product_5  \\\n",
       "0   2.500146   2.483446   2.545823   2.430334   2.418919   2.560656   \n",
       "1   2.499887   2.491398   2.550073   2.457689   2.431810   2.535469   \n",
       "2   2.501173   2.499612   2.542211   2.478822   2.458534   2.520449   \n",
       "3   2.498349   2.495599   2.544780   2.485741   2.461424   2.529714   \n",
       "4   2.500267   2.501994   2.526376   2.500614   2.485562   2.520482   \n",
       "5   2.499891   2.501807   2.535126   2.494161   2.487670   2.521840   \n",
       "6   2.499861   2.500766   2.527890   2.504906   2.499042   2.527144   \n",
       "7   2.500702   2.502642   2.527015   2.501036   2.490412   2.520514   \n",
       "8   2.497859   2.490908   2.552183   2.464734   2.440772   2.537108   \n",
       "9   2.501272   2.502552   2.527914   2.497245   2.485324   2.520929   \n",
       "\n",
       "   Product_6  Product_7  Product_8  Product_9  ...  Product_5513  \\\n",
       "0   2.573986   2.335510   2.598732   2.702518  ...      2.423273   \n",
       "1   2.550894   2.375421   2.583809   2.640193  ...      2.455241   \n",
       "2   2.532574   2.424906   2.574032   2.589723  ...      2.480690   \n",
       "3   2.545595   2.413802   2.555633   2.597207  ...      2.473763   \n",
       "4   2.522143   2.472629   2.543218   2.556836  ...      2.498905   \n",
       "5   2.527334   2.471783   2.548614   2.571597  ...      2.493630   \n",
       "6   2.529876   2.480635   2.533073   2.563903  ...      2.500032   \n",
       "7   2.523102   2.477982   2.544531   2.554900  ...      2.500430   \n",
       "8   2.552458   2.385446   2.570580   2.637084  ...      2.450376   \n",
       "9   2.523123   2.473194   2.547755   2.557995  ...      2.499270   \n",
       "\n",
       "   Product_5514  Product_5515  Product_5516  Product_5517  Product_5518  \\\n",
       "0      2.607817      2.725234      2.447518      2.378659      2.194475   \n",
       "1      2.574931      2.679621      2.489161      2.409468      2.319759   \n",
       "2      2.550498      2.616248      2.502605      2.441787      2.390606   \n",
       "3      2.547965      2.631993      2.505312      2.443451      2.412105   \n",
       "4      2.526371      2.564127      2.508245      2.482816      2.474928   \n",
       "5      2.527498      2.582798      2.511891      2.470459      2.461424   \n",
       "6      2.526649      2.567344      2.513847      2.485996      2.490911   \n",
       "7      2.525951      2.560450      2.510499      2.479122      2.477944   \n",
       "8      2.561986      2.678980      2.493556      2.414977      2.342915   \n",
       "9      2.529712      2.568072      2.510451      2.474978      2.469494   \n",
       "\n",
       "   Product_5519  Product_5520  Product_5521  Product_5522  \n",
       "0      2.133805      2.163052      2.176616      2.261167  \n",
       "1      2.261487      2.276363      2.296059      2.308084  \n",
       "2      2.357120      2.358881      2.381980      2.369528  \n",
       "3      2.353026      2.371088      2.388363      2.356236  \n",
       "4      2.453501      2.452916      2.480318      2.416341  \n",
       "5      2.435823      2.438302      2.471032      2.404418  \n",
       "6      2.462953      2.467316      2.493640      2.420988  \n",
       "7      2.457366      2.457349      2.483157      2.417714  \n",
       "8      2.284178      2.301177      2.324464      2.306963  \n",
       "9      2.449777      2.447256      2.476366      2.412649  \n",
       "\n",
       "[10 rows x 5523 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z=torch.randn(10,100)\n",
    "# synthetic_profiles=generator(z)\n",
    "# original_dim_synthetic_profiles = synthetic_profiles @ torch.tensor(svd.components_, dtype=torch.float32)\n",
    "# original_dim_synthetic_profiles = original_dim_synthetic_profiles.detach().numpy()\n",
    "# scaled_profiles = (original_dim_synthetic_profiles + 1) * 2 # Maps from [-1, 1] to [1, 5]\n",
    "# threshold = 1\n",
    "# scaled_profiles[scaled_profiles < threshold] = 0\n",
    "# synthetic_profiles_df = pd.DataFrame(scaled_profiles, columns=[f\"Product_{i}\" for i in range(product_embeddings.shape[1])])\n",
    "# synthetic_profiles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dim = 5523   # Product dimension (number of products)\n",
    "latent_dim = 64    # Latent space dimension for VAE\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEGenerator(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAEGenerator, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim * 2)  # For both mean and log-variance\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim),\n",
    "            nn.Sigmoid()  # Ratings scaled between 0 and 1\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = h[:, :latent_dim], h[:, latent_dim:]\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_generator = VAEGenerator(input_dim, latent_dim)\n",
    "discriminator = Discriminator(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vae_optimizer = optim.Adam(vae_generator.parameters(), lr=learning_rate)\n",
    "generator_optimizer = optim.Adam(vae_generator.decoder.parameters(), lr=learning_rate)  # Separate optimizer for generator\n",
    "\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_criterion = nn.MSELoss()\n",
    "adversarial_criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "user_product_matrix = torch.tensor(user_item.values,dtype=torch.float32)\n",
    "dataset = TensorDataset(user_product_matrix)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\autograd\\graph.py:769: UserWarning: Error detected in AddmmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\asyncio\\windows_events.py\", line 316, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\asyncio\\events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Mohamed Mosaad\\AppData\\Local\\Temp\\ipykernel_17560\\1453064830.py\", line 13, in <module>\n",
      "    generated_profile, mu, log_var = vae_generator(batch)\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\Mohamed Mosaad\\AppData\\Local\\Temp\\ipykernel_17560\\2512408371.py\", line 33, in forward\n",
      "    return self.decoder(z), mu, log_var\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 219, in forward\n",
      "    input = module(input)\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 117, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      " (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:116.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [256, 5523]], which is output 0 of AsStridedBackward0, is at version 3; expected version 2 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[187], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m     generator_loss \u001b[38;5;241m=\u001b[39m adversarial_criterion(discriminator(cloned_generated_profile), real_labels)\n\u001b[0;32m     54\u001b[0m     generator_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 55\u001b[0m     \u001b[43mgenerator_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure the computation graph is clean\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     generator_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], VAE Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvae_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, D Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiscriminator_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, G Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerator_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mohamed Mosaad\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [256, 5523]], which is output 0 of AsStridedBackward0, is at version 3; expected version 2 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Enable anomaly detection for debugging\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (batch,) in enumerate(dataloader):\n",
    "        batch = batch.squeeze(1)\n",
    "        \n",
    "        # ================== Train VAE (Encoder-Decoder) ==================\n",
    "        # Generate profiles from the VAE\n",
    "        generated_profile, mu, log_var = vae_generator(batch)\n",
    "\n",
    "        # Create masks to avoid in-place operations\n",
    "        mask = (batch != 0).clone()  # Mask for valid ratings\n",
    "\n",
    "        # Ensure the tensors used for loss calculations are not modified in place\n",
    "        masked_generated_profile = generated_profile * mask\n",
    "        masked_batch = batch * mask\n",
    "\n",
    "        # Compute the reconstruction loss\n",
    "        reconstruction_loss = reconstruction_criterion(masked_generated_profile.clone(), masked_batch.clone())\n",
    "\n",
    "        # KL Divergence\n",
    "        kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) / batch.size(0)\n",
    "\n",
    "        # Total VAE loss\n",
    "        vae_loss = reconstruction_loss + kl_divergence\n",
    "        vae_optimizer.zero_grad()\n",
    "        vae_loss.backward(retain_graph=True)  # Retain the graph for the next backward pass\n",
    "        vae_optimizer.step()\n",
    "\n",
    "        # ================== Train Discriminator ==================\n",
    "        real_labels = torch.ones(batch.size(0), 1)\n",
    "        fake_labels = torch.zeros(batch.size(0), 1)\n",
    "\n",
    "        # Discriminator loss on real profiles\n",
    "        real_loss = adversarial_criterion(discriminator(batch.detach()), real_labels)\n",
    "\n",
    "        # Discriminator loss on generated (fake) profiles\n",
    "        fake_loss = adversarial_criterion(discriminator(generated_profile.detach()), fake_labels)\n",
    "        \n",
    "        # Total discriminator loss\n",
    "        discriminator_loss = real_loss + fake_loss\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        # ================== Train Generator (VAE Decoder) ==================\n",
    "        # Clone generated_profile to avoid in-place operations\n",
    "        cloned_generated_profile = generated_profile.clone()  # Clone here\n",
    "        generator_loss = adversarial_criterion(discriminator(cloned_generated_profile), real_labels)\n",
    "        generator_optimizer.zero_grad()\n",
    "        generator_loss.backward()  # Ensure the computation graph is clean\n",
    "        generator_optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], VAE Loss: {vae_loss.item()}, D Loss: {discriminator_loss.item()}, G Loss: {generator_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "num_users = 1466\n",
    "num_products = 5523\n",
    "batch_size = 128\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, num_products),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(num_products, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1000 [D loss: 0.94769287109375] [G loss: 1.817845344543457]\n",
      "100/1000 [D loss: 0.31211620569229126] [G loss: 1.466598629951477]\n",
      "200/1000 [D loss: 0.046661920845508575] [G loss: 4.27742338180542]\n",
      "300/1000 [D loss: 0.019628623500466347] [G loss: 4.773027420043945]\n",
      "400/1000 [D loss: 0.004255291074514389] [G loss: 5.7544097900390625]\n",
      "500/1000 [D loss: 0.014859076589345932] [G loss: 5.991272926330566]\n",
      "600/1000 [D loss: 0.00381445768289268] [G loss: 7.077122688293457]\n",
      "700/1000 [D loss: 0.0013609391171485186] [G loss: 6.921176433563232]\n",
      "800/1000 [D loss: 0.002675817348062992] [G loss: 6.356795787811279]\n",
      "900/1000 [D loss: 0.0009479106520302594] [G loss: 6.762637615203857]\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    real_profiles = torch.tensor(user_item.values,dtype=torch.float32) # Replace with your actual data\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, num_users, batch_size):\n",
    "            real_data = real_profiles[i:i+batch_size]\n",
    "            batch_size_actual = real_data.size(0)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            noise = torch.randn(batch_size_actual, latent_dim)\n",
    "            fake_data = generator(noise)\n",
    "            real_labels = torch.ones(batch_size_actual, 1)\n",
    "            fake_labels = torch.zeros(batch_size_actual, 1)\n",
    "            outputs_real = discriminator(real_data)\n",
    "            outputs_fake = discriminator(fake_data)\n",
    "            d_loss_real = criterion(outputs_real, real_labels)\n",
    "            d_loss_fake = criterion(outputs_fake, fake_labels)\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            optimizer_D.zero_grad()\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            # Train Generator\n",
    "            noise = torch.randn(batch_size_actual, latent_dim)\n",
    "            fake_data = generator(noise)\n",
    "            outputs = discriminator(fake_data)\n",
    "            g_loss = criterion(outputs, real_labels)\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"{epoch}/{epochs} [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "\n",
    "# Train the GAN\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(num_users, latent_dim)\n",
    "synthetic_profiles = generator(noise).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_profiles = (synthetic_profiles - synthetic_profiles.min()) / (synthetic_profiles.max() - synthetic_profiles.min()) * 3 + 1\n",
    "synthetic_df = pd.DataFrame(synthetic_profiles, columns=[f'Product_{i+1}' for i in range(num_products)])\n",
    "synthetic_df['User_ID'] = [f'Synthetic_User_{i+1}' for i in range(num_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_1</th>\n",
       "      <th>Product_2</th>\n",
       "      <th>Product_3</th>\n",
       "      <th>Product_4</th>\n",
       "      <th>Product_5</th>\n",
       "      <th>Product_6</th>\n",
       "      <th>Product_7</th>\n",
       "      <th>Product_8</th>\n",
       "      <th>Product_9</th>\n",
       "      <th>Product_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Product_5515</th>\n",
       "      <th>Product_5516</th>\n",
       "      <th>Product_5517</th>\n",
       "      <th>Product_5518</th>\n",
       "      <th>Product_5519</th>\n",
       "      <th>Product_5520</th>\n",
       "      <th>Product_5521</th>\n",
       "      <th>Product_5522</th>\n",
       "      <th>Product_5523</th>\n",
       "      <th>User_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.314103e-07</td>\n",
       "      <td>2.819219e-07</td>\n",
       "      <td>1.251384e-07</td>\n",
       "      <td>3.999458e-07</td>\n",
       "      <td>7.756937e-08</td>\n",
       "      <td>1.359688e-06</td>\n",
       "      <td>1.066595e-06</td>\n",
       "      <td>1.617725e-06</td>\n",
       "      <td>7.807677e-07</td>\n",
       "      <td>6.889765e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>4.946278e-07</td>\n",
       "      <td>6.981419e-08</td>\n",
       "      <td>3.860194e-05</td>\n",
       "      <td>4.503882e-08</td>\n",
       "      <td>3.806463e-07</td>\n",
       "      <td>6.922241e-06</td>\n",
       "      <td>2.881597e-06</td>\n",
       "      <td>2.305486e-10</td>\n",
       "      <td>Synthetic_User_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.092474e-08</td>\n",
       "      <td>6.471997e-08</td>\n",
       "      <td>1.349108e-09</td>\n",
       "      <td>6.644839e-08</td>\n",
       "      <td>1.938961e-09</td>\n",
       "      <td>7.811246e-08</td>\n",
       "      <td>5.560947e-07</td>\n",
       "      <td>1.227078e-07</td>\n",
       "      <td>1.898383e-08</td>\n",
       "      <td>1.676980e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>5.048454e-10</td>\n",
       "      <td>1.523311e-09</td>\n",
       "      <td>4.482919e-07</td>\n",
       "      <td>3.620554e-11</td>\n",
       "      <td>3.679526e-07</td>\n",
       "      <td>1.217089e-07</td>\n",
       "      <td>7.220895e-10</td>\n",
       "      <td>4.787342e-11</td>\n",
       "      <td>Synthetic_User_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.759703e-09</td>\n",
       "      <td>5.822406e-10</td>\n",
       "      <td>1.328673e-10</td>\n",
       "      <td>4.532034e-12</td>\n",
       "      <td>1.573813e-10</td>\n",
       "      <td>4.341150e-08</td>\n",
       "      <td>5.580370e-07</td>\n",
       "      <td>1.658891e-08</td>\n",
       "      <td>2.532320e-08</td>\n",
       "      <td>1.276538e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>7.944859e-04</td>\n",
       "      <td>3.886457e-08</td>\n",
       "      <td>1.452128e-04</td>\n",
       "      <td>6.502775e-03</td>\n",
       "      <td>2.571572e-06</td>\n",
       "      <td>1.459723e-05</td>\n",
       "      <td>1.987746e-02</td>\n",
       "      <td>2.281291e-11</td>\n",
       "      <td>Synthetic_User_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.373038e-08</td>\n",
       "      <td>5.185706e-08</td>\n",
       "      <td>1.529121e-09</td>\n",
       "      <td>9.170477e-09</td>\n",
       "      <td>2.282295e-08</td>\n",
       "      <td>2.235196e-07</td>\n",
       "      <td>1.994207e-07</td>\n",
       "      <td>6.612472e-07</td>\n",
       "      <td>4.341838e-07</td>\n",
       "      <td>3.656787e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>7.376718e-08</td>\n",
       "      <td>1.211395e-08</td>\n",
       "      <td>5.371261e-05</td>\n",
       "      <td>2.589847e-08</td>\n",
       "      <td>1.244156e-06</td>\n",
       "      <td>7.686333e-06</td>\n",
       "      <td>1.078753e-06</td>\n",
       "      <td>3.759486e-10</td>\n",
       "      <td>Synthetic_User_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.856317e-07</td>\n",
       "      <td>5.703386e-08</td>\n",
       "      <td>1.767792e-10</td>\n",
       "      <td>1.108913e-04</td>\n",
       "      <td>1.088164e-08</td>\n",
       "      <td>4.662554e-09</td>\n",
       "      <td>3.551229e-08</td>\n",
       "      <td>2.912249e-06</td>\n",
       "      <td>1.549095e-07</td>\n",
       "      <td>2.411328e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>2.701955e-17</td>\n",
       "      <td>9.478278e-11</td>\n",
       "      <td>2.375169e-11</td>\n",
       "      <td>2.365395e-18</td>\n",
       "      <td>7.379301e-10</td>\n",
       "      <td>1.668385e-08</td>\n",
       "      <td>3.816288e-17</td>\n",
       "      <td>4.008376e-14</td>\n",
       "      <td>Synthetic_User_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>7.977912e-07</td>\n",
       "      <td>5.487592e-08</td>\n",
       "      <td>5.396907e-10</td>\n",
       "      <td>2.426345e-09</td>\n",
       "      <td>2.703705e-08</td>\n",
       "      <td>1.275132e-08</td>\n",
       "      <td>1.382912e-07</td>\n",
       "      <td>7.640645e-07</td>\n",
       "      <td>5.804250e-07</td>\n",
       "      <td>1.020193e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2.241783e-08</td>\n",
       "      <td>1.714673e-08</td>\n",
       "      <td>2.898357e-05</td>\n",
       "      <td>1.739481e-07</td>\n",
       "      <td>3.533750e-07</td>\n",
       "      <td>1.024256e-05</td>\n",
       "      <td>1.178962e-06</td>\n",
       "      <td>1.239422e-11</td>\n",
       "      <td>Synthetic_User_1462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>2.235613e-08</td>\n",
       "      <td>3.889928e-08</td>\n",
       "      <td>1.305751e-09</td>\n",
       "      <td>1.066955e-09</td>\n",
       "      <td>4.586323e-08</td>\n",
       "      <td>1.953462e-08</td>\n",
       "      <td>8.294174e-07</td>\n",
       "      <td>1.303684e-07</td>\n",
       "      <td>5.482424e-08</td>\n",
       "      <td>2.060432e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>5.991798e-05</td>\n",
       "      <td>1.885705e-08</td>\n",
       "      <td>2.129597e-05</td>\n",
       "      <td>4.030798e-06</td>\n",
       "      <td>6.059844e-07</td>\n",
       "      <td>4.538910e-06</td>\n",
       "      <td>5.490053e-05</td>\n",
       "      <td>1.479544e-10</td>\n",
       "      <td>Synthetic_User_1463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>3.447666e-07</td>\n",
       "      <td>1.143016e-08</td>\n",
       "      <td>1.452509e-09</td>\n",
       "      <td>4.623503e-06</td>\n",
       "      <td>5.857008e-08</td>\n",
       "      <td>4.763463e-07</td>\n",
       "      <td>2.305799e-07</td>\n",
       "      <td>7.776520e-06</td>\n",
       "      <td>4.491118e-07</td>\n",
       "      <td>4.033870e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>5.755569e-11</td>\n",
       "      <td>3.733926e-09</td>\n",
       "      <td>2.261483e-07</td>\n",
       "      <td>7.044178e-12</td>\n",
       "      <td>3.372175e-07</td>\n",
       "      <td>1.019219e-06</td>\n",
       "      <td>3.334882e-11</td>\n",
       "      <td>6.766665e-11</td>\n",
       "      <td>Synthetic_User_1464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>1.425405e-06</td>\n",
       "      <td>1.326070e-06</td>\n",
       "      <td>7.631009e-09</td>\n",
       "      <td>7.783332e-05</td>\n",
       "      <td>2.422495e-07</td>\n",
       "      <td>2.560887e-06</td>\n",
       "      <td>4.762432e-06</td>\n",
       "      <td>3.281802e-05</td>\n",
       "      <td>4.866965e-07</td>\n",
       "      <td>1.132047e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>1.058451e-11</td>\n",
       "      <td>4.568850e-09</td>\n",
       "      <td>3.692127e-07</td>\n",
       "      <td>7.569746e-13</td>\n",
       "      <td>1.545094e-08</td>\n",
       "      <td>1.499307e-06</td>\n",
       "      <td>1.012033e-11</td>\n",
       "      <td>4.286489e-10</td>\n",
       "      <td>Synthetic_User_1465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>1.037703e-06</td>\n",
       "      <td>7.625359e-07</td>\n",
       "      <td>2.720505e-09</td>\n",
       "      <td>1.277664e-05</td>\n",
       "      <td>8.048403e-08</td>\n",
       "      <td>3.786636e-07</td>\n",
       "      <td>5.836867e-06</td>\n",
       "      <td>1.803217e-05</td>\n",
       "      <td>6.087467e-07</td>\n",
       "      <td>1.249538e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>2.441447e-09</td>\n",
       "      <td>3.158575e-09</td>\n",
       "      <td>1.704739e-06</td>\n",
       "      <td>4.584349e-11</td>\n",
       "      <td>9.352726e-08</td>\n",
       "      <td>4.460400e-06</td>\n",
       "      <td>8.446479e-10</td>\n",
       "      <td>1.620751e-10</td>\n",
       "      <td>Synthetic_User_1466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1466 rows × 5524 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Product_1     Product_2     Product_3     Product_4     Product_5  \\\n",
       "0     5.314103e-07  2.819219e-07  1.251384e-07  3.999458e-07  7.756937e-08   \n",
       "1     8.092474e-08  6.471997e-08  1.349108e-09  6.644839e-08  1.938961e-09   \n",
       "2     3.759703e-09  5.822406e-10  1.328673e-10  4.532034e-12  1.573813e-10   \n",
       "3     8.373038e-08  5.185706e-08  1.529121e-09  9.170477e-09  2.282295e-08   \n",
       "4     6.856317e-07  5.703386e-08  1.767792e-10  1.108913e-04  1.088164e-08   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1461  7.977912e-07  5.487592e-08  5.396907e-10  2.426345e-09  2.703705e-08   \n",
       "1462  2.235613e-08  3.889928e-08  1.305751e-09  1.066955e-09  4.586323e-08   \n",
       "1463  3.447666e-07  1.143016e-08  1.452509e-09  4.623503e-06  5.857008e-08   \n",
       "1464  1.425405e-06  1.326070e-06  7.631009e-09  7.783332e-05  2.422495e-07   \n",
       "1465  1.037703e-06  7.625359e-07  2.720505e-09  1.277664e-05  8.048403e-08   \n",
       "\n",
       "         Product_6     Product_7     Product_8     Product_9    Product_10  \\\n",
       "0     1.359688e-06  1.066595e-06  1.617725e-06  7.807677e-07  6.889765e-07   \n",
       "1     7.811246e-08  5.560947e-07  1.227078e-07  1.898383e-08  1.676980e-06   \n",
       "2     4.341150e-08  5.580370e-07  1.658891e-08  2.532320e-08  1.276538e-08   \n",
       "3     2.235196e-07  1.994207e-07  6.612472e-07  4.341838e-07  3.656787e-07   \n",
       "4     4.662554e-09  3.551229e-08  2.912249e-06  1.549095e-07  2.411328e-06   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1461  1.275132e-08  1.382912e-07  7.640645e-07  5.804250e-07  1.020193e-06   \n",
       "1462  1.953462e-08  8.294174e-07  1.303684e-07  5.482424e-08  2.060432e-07   \n",
       "1463  4.763463e-07  2.305799e-07  7.776520e-06  4.491118e-07  4.033870e-06   \n",
       "1464  2.560887e-06  4.762432e-06  3.281802e-05  4.866965e-07  1.132047e-05   \n",
       "1465  3.786636e-07  5.836867e-06  1.803217e-05  6.087467e-07  1.249538e-06   \n",
       "\n",
       "      ...  Product_5515  Product_5516  Product_5517  Product_5518  \\\n",
       "0     ...      0.000081  4.946278e-07  6.981419e-08  3.860194e-05   \n",
       "1     ...      0.000025  5.048454e-10  1.523311e-09  4.482919e-07   \n",
       "2     ...      0.000006  7.944859e-04  3.886457e-08  1.452128e-04   \n",
       "3     ...      0.000134  7.376718e-08  1.211395e-08  5.371261e-05   \n",
       "4     ...      0.000002  2.701955e-17  9.478278e-11  2.375169e-11   \n",
       "...   ...           ...           ...           ...           ...   \n",
       "1461  ...      0.000004  2.241783e-08  1.714673e-08  2.898357e-05   \n",
       "1462  ...      0.000047  5.991798e-05  1.885705e-08  2.129597e-05   \n",
       "1463  ...      0.000014  5.755569e-11  3.733926e-09  2.261483e-07   \n",
       "1464  ...      0.000046  1.058451e-11  4.568850e-09  3.692127e-07   \n",
       "1465  ...      0.000127  2.441447e-09  3.158575e-09  1.704739e-06   \n",
       "\n",
       "      Product_5519  Product_5520  Product_5521  Product_5522  Product_5523  \\\n",
       "0     4.503882e-08  3.806463e-07  6.922241e-06  2.881597e-06  2.305486e-10   \n",
       "1     3.620554e-11  3.679526e-07  1.217089e-07  7.220895e-10  4.787342e-11   \n",
       "2     6.502775e-03  2.571572e-06  1.459723e-05  1.987746e-02  2.281291e-11   \n",
       "3     2.589847e-08  1.244156e-06  7.686333e-06  1.078753e-06  3.759486e-10   \n",
       "4     2.365395e-18  7.379301e-10  1.668385e-08  3.816288e-17  4.008376e-14   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1461  1.739481e-07  3.533750e-07  1.024256e-05  1.178962e-06  1.239422e-11   \n",
       "1462  4.030798e-06  6.059844e-07  4.538910e-06  5.490053e-05  1.479544e-10   \n",
       "1463  7.044178e-12  3.372175e-07  1.019219e-06  3.334882e-11  6.766665e-11   \n",
       "1464  7.569746e-13  1.545094e-08  1.499307e-06  1.012033e-11  4.286489e-10   \n",
       "1465  4.584349e-11  9.352726e-08  4.460400e-06  8.446479e-10  1.620751e-10   \n",
       "\n",
       "                  User_ID  \n",
       "0        Synthetic_User_1  \n",
       "1        Synthetic_User_2  \n",
       "2        Synthetic_User_3  \n",
       "3        Synthetic_User_4  \n",
       "4        Synthetic_User_5  \n",
       "...                   ...  \n",
       "1461  Synthetic_User_1462  \n",
       "1462  Synthetic_User_1463  \n",
       "1463  Synthetic_User_1464  \n",
       "1464  Synthetic_User_1465  \n",
       "1465  Synthetic_User_1466  \n",
       "\n",
       "[1466 rows x 5524 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_1</th>\n",
       "      <th>Product_2</th>\n",
       "      <th>Product_3</th>\n",
       "      <th>Product_4</th>\n",
       "      <th>Product_5</th>\n",
       "      <th>Product_6</th>\n",
       "      <th>Product_7</th>\n",
       "      <th>Product_8</th>\n",
       "      <th>Product_9</th>\n",
       "      <th>Product_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Product_5515</th>\n",
       "      <th>Product_5516</th>\n",
       "      <th>Product_5517</th>\n",
       "      <th>Product_5518</th>\n",
       "      <th>Product_5519</th>\n",
       "      <th>Product_5520</th>\n",
       "      <th>Product_5521</th>\n",
       "      <th>Product_5522</th>\n",
       "      <th>Product_5523</th>\n",
       "      <th>User_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000002</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000081</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000007</td>\n",
       "      <td>1.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Synthetic_User_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000025</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Synthetic_User_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000006</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000145</td>\n",
       "      <td>1.006503</td>\n",
       "      <td>1.000003</td>\n",
       "      <td>1.000015</td>\n",
       "      <td>1.019877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Synthetic_User_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000134</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000054</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>1.000008</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Synthetic_User_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000002</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>1.000001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000029</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000047</td>\n",
       "      <td>1.00006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>1.000005</td>\n",
       "      <td>1.000055</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Synthetic_User_1463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000014</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Synthetic_User_1464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000003</td>\n",
       "      <td>1.000005</td>\n",
       "      <td>1.000033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000046</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Synthetic_User_1465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>1.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000013</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000127</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Synthetic_User_1466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1466 rows × 5524 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Product_1  Product_2  Product_3  Product_4  Product_5  Product_6  \\\n",
       "0      1.000000   1.000000        1.0   0.000000        1.0   0.000000   \n",
       "1      1.000000   1.000000        1.0   0.000000        1.0   0.000000   \n",
       "2      1.000000   1.000000        1.0   0.000000        0.0   1.000000   \n",
       "3      1.000000   1.000000        1.0   0.000000        1.0   1.000000   \n",
       "4      1.000001   1.000000        1.0   1.000111        1.0   1.000000   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1461   1.000001   1.000000        1.0   0.000000        1.0   1.000000   \n",
       "1462   1.000000   1.000000        1.0   1.000000        1.0   1.000000   \n",
       "1463   1.000000   1.000000        1.0   0.000000        1.0   1.000000   \n",
       "1464   0.000000   1.000001        1.0   1.000078        1.0   1.000003   \n",
       "1465   1.000001   0.000000        1.0   1.000013        1.0   0.000000   \n",
       "\n",
       "      Product_7  Product_8  Product_9  Product_10  ...  Product_5515  \\\n",
       "0      0.000000   1.000002   1.000001    1.000001  ...      1.000081   \n",
       "1      1.000001   1.000000   1.000000    1.000002  ...      1.000025   \n",
       "2      0.000000   1.000000   1.000000    0.000000  ...      1.000006   \n",
       "3      1.000000   1.000001   0.000000    1.000000  ...      1.000134   \n",
       "4      0.000000   1.000003   0.000000    1.000002  ...      1.000002   \n",
       "...         ...        ...        ...         ...  ...           ...   \n",
       "1461   1.000000   1.000001   1.000001    1.000001  ...      1.000004   \n",
       "1462   1.000001   1.000000   1.000000    0.000000  ...      1.000047   \n",
       "1463   1.000000   1.000008   0.000000    1.000004  ...      1.000014   \n",
       "1464   1.000005   1.000033   0.000000    1.000011  ...      1.000046   \n",
       "1465   1.000006   0.000000   0.000000    1.000001  ...      1.000127   \n",
       "\n",
       "      Product_5516  Product_5517  Product_5518  Product_5519  Product_5520  \\\n",
       "0          1.00000           1.0      0.000000      1.000000      1.000000   \n",
       "1          0.00000           0.0      0.000000      1.000000      1.000000   \n",
       "2          0.00000           1.0      1.000145      1.006503      1.000003   \n",
       "3          1.00000           1.0      1.000054      1.000000      1.000001   \n",
       "4          1.00000           1.0      1.000000      1.000000      1.000000   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1461       1.00000           1.0      1.000029      1.000000      0.000000   \n",
       "1462       1.00006           1.0      1.000021      0.000000      1.000001   \n",
       "1463       1.00000           1.0      0.000000      1.000000      1.000000   \n",
       "1464       1.00000           1.0      1.000000      1.000000      1.000000   \n",
       "1465       1.00000           1.0      1.000002      1.000000      1.000000   \n",
       "\n",
       "      Product_5521  Product_5522  Product_5523              User_ID  \n",
       "0         1.000007      1.000003           0.0     Synthetic_User_1  \n",
       "1         1.000000      1.000000           1.0     Synthetic_User_2  \n",
       "2         1.000015      1.019877           0.0     Synthetic_User_3  \n",
       "3         1.000008      1.000001           1.0     Synthetic_User_4  \n",
       "4         1.000000      1.000000           1.0                       \n",
       "...            ...           ...           ...                  ...  \n",
       "1461      0.000000      1.000001           1.0                       \n",
       "1462      1.000005      1.000055           1.0  Synthetic_User_1463  \n",
       "1463      1.000001      1.000000           1.0  Synthetic_User_1464  \n",
       "1464      1.000002      1.000000           1.0  Synthetic_User_1465  \n",
       "1465      0.000000      0.000000           0.0  Synthetic_User_1466  \n",
       "\n",
       "[1466 rows x 5524 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_profiles = (synthetic_profiles+1)   # Scale to range [1, 4]\n",
    "synthetic_df = pd.DataFrame(synthetic_profiles, columns=[f'Product_{i+1}' for i in range(num_products)])\n",
    "synthetic_df['User_ID'] = [f'Synthetic_User_{i+1}' for i in range(num_users)]\n",
    "\n",
    "# Introduce sparsity (set some ratings to zero)\n",
    "sparsity_mask = np.random.rand(*synthetic_df.shape) < 0.8  # Adjust 0.8 for desired sparsity level\n",
    "synthetic_df = synthetic_df * sparsity_mask\n",
    "pd.DataFrame(synthetic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
